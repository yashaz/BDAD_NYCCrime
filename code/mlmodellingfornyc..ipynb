{
  "metadata": {
    "name": "mlmodellingfornyc",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "spark.sql(\"USE ss14431_nyu_edu\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\nval calls \u003d spark.sql(\"SELECT CREATE_DATE,NYPD_PCT_CD, count(*) as COUNT_CALLS FROM nyc_911_calls group by CREATE_DATE,NYPD_PCT_CD\")\n\nval complaints \u003d spark.sql(\"select trim(`COMPLAINT DATE`) AS COMPLAINT_DATE, `NYPD PRETINCT` AS COMPLAINT_PCT, count(*) as COUNT_COMPL from nyc_complaints group by `COMPLAINT DATE`,`NYPD PRETINCT`\")\n\nval arrests \u003d spark.sql(\"select ARREST_DATE, ARREST_PRECINCT, count(*) as COUNT_ARREST from nyc_arrest group by ARREST_DATE, ARREST_PRECINCT\")\n\nval merged \u003d complaints.join(calls, complaints(\"COMPLAINT_DATE\") \u003d\u003d\u003d calls(\"CREATE_DATE\") \u0026\u0026 complaints(\"COMPLAINT_PCT\") \u003d\u003d\u003d calls(\"NYPD_PCT_CD\"),\"inner\").join(arrests, complaints(\"COMPLAINT_DATE\") \u003d\u003d\u003d arrests(\"ARREST_DATE\") \u0026\u0026 complaints(\"COMPLAINT_PCT\") \u003d\u003d\u003d arrests(\"ARREST_PRECINCT\"),\"inner\").select(\"ARREST_DATE\",\"ARREST_PRECINCT\",\"COUNT_CALLS\",\"COUNT_COMPL\",\"COUNT_ARREST\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\nval calls \u003d spark.sql(\"SELECT CREATE_DATE,NYPD_PCT_CD, count(*) as COUNT_CALLS FROM nyc_911_calls group by CREATE_DATE,NYPD_PCT_CD\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val complaints \u003d spark.sql(\"select trim(`COMPLAINT DATE`) AS COMPLAINT_DATE, `NYPD PRETINCT` AS COMPLAINT_PCT, count(*) as COUNT_COMPL from nyc_complaints group by `COMPLAINT_DATE`,`NYPD PRETINCT`\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val complaints \u003d spark.sql(\"select trim(`COMPLAINT DATE`) AS COMPLAINT_DATE, `NYPD PRETINCT` AS COMPLAINT_PCT, count(*) as COUNT_COMPL from nyc_complaints group by `COMPLAINT DATE`,`NYPD PRETINCT`\")"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val filePath \u003d \"/user/ss14431_nyu_edu/bdadProject/data.parquet\"\nval data \u003d spark.read.parquet(filePath)\nz.show(data)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val Array(trainDF, testDF) \u003d data.randomSplit(Array(.8, .2), seed\u003d42)\nprintln(f\"There are ${trainDF.cache().count()} rows in the training set, and ${testDF.cache().count()} in the test set\")"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.VectorIndexer\nimport org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}\nimport org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}\nimport org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.feature.RFormula\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val categoricalCols \u003d trainDF.dtypes.filter(_._2 \u003d\u003d \"StringType\").map(_._1)\nval indexOutputCols \u003d categoricalCols.map(_ + \"_index\")\nval oheOutputCols \u003d categoricalCols.map(_ + \"_OHE\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val stringIndexer \u003d new StringIndexer().setInputCols(categoricalCols).setOutputCols(indexOutputCols).setHandleInvalid(\"skip\")\nval oheEncoder \u003d new OneHotEncoder().setInputCols(indexOutputCols).setOutputCols(oheOutputCols)\nval res1 \u003d desiredNumericCols.slice(0,3)\nval res2 \u003d desiredNumericCols.drop(4)\nval numericCols \u003d res1 ++ res2\nval assemblerInputs \u003d oheOutputCols ++ numericCols\nval vecAssembler \u003d new VectorAssembler().setInputCols(assemblerInputs).setOutputCol(\"features\")\nval rFormula \u003d new RFormula().setFormula(\"COUNT_ARREST ~ .\").setFeaturesCol(\"features\").setLabelCol(\"COUNT_ARREST\").setHandleInvalid(\"skip\")"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val lr \u003d new LinearRegression().setLabelCol(\"COUNT_ARREST\").setFeaturesCol(\"features\")\nval pipeline \u003d new Pipeline().setStages(Array(rFormula,stringIndexer, oheEncoder, vecAssembler, lr))\nval pipelineModel \u003d pipeline.fit(trainDF)\nval predDF \u003d pipelineModel.transform(testDF)\nz.show(predDF.select(\"features\", \"COUNT_ARREST\", \"prediction\"))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val desiredNumericCols \u003d trainDF.dtypes.filter{case (field, dataType) \u003d\u003e dataType !\u003d \"StringType\" \u0026\u0026 field !\u003d \"DAY\"}.map(_._1)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val stringIndexer \u003d new StringIndexer().setInputCols(categoricalCols).setOutputCols(indexOutputCols).setHandleInvalid(\"skip\")\nval oheEncoder \u003d new OneHotEncoder().setInputCols(indexOutputCols).setOutputCols(oheOutputCols)\nval res1 \u003d desiredNumericCols.slice(0,3)\nval res2 \u003d desiredNumericCols.drop(4)\nval numericCols \u003d res1 ++ res2\nval assemblerInputs \u003d oheOutputCols ++ numericCols\nval vecAssembler \u003d new VectorAssembler().setInputCols(assemblerInputs).setOutputCol(\"features\")\nval rFormula \u003d new RFormula().setFormula(\"COUNT_ARREST ~ .\").setFeaturesCol(\"features\").setLabelCol(\"COUNT_ARREST\").setHandleInvalid(\"skip\")"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.functions.udf\nimport java.text.SimpleDateFormat"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val stringIndexer \u003d new StringIndexer().setInputCols(categoricalCols).setOutputCols(indexOutputCols).setHandleInvalid(\"skip\")\nval oheEncoder \u003d new OneHotEncoder().setInputCols(indexOutputCols).setOutputCols(oheOutputCols)\nval res1 \u003d desiredNumericCols.slice(0,3)\nval res2 \u003d desiredNumericCols.drop(4)\nval numericCols \u003d res1 ++ res2\nval assemblerInputs \u003d oheOutputCols ++ numericCols\nval vecAssembler \u003d new VectorAssembler().setInputCols(assemblerInputs).setOutputCol(\"features\")\nval rFormula \u003d new RFormula().setFormula(\"COUNT_ARREST ~ .\").setFeaturesCol(\"features\").setLabelCol(\"COUNT_ARREST\").setHandleInvalid(\"skip\")"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.feature.VectorAssembler"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val stringIndexer \u003d new StringIndexer().setInputCols(categoricalCols).setOutputCols(indexOutputCols).setHandleInvalid(\"skip\")\nval oheEncoder \u003d new OneHotEncoder().setInputCols(indexOutputCols).setOutputCols(oheOutputCols)\nval res1 \u003d desiredNumericCols.slice(0,3)\nval res2 \u003d desiredNumericCols.drop(4)\nval numericCols \u003d res1 ++ res2\nval assemblerInputs \u003d oheOutputCols ++ numericCols\nval vecAssembler \u003d new VectorAssembler().setInputCols(assemblerInputs).setOutputCol(\"features\")\nval rFormula \u003d new RFormula().setFormula(\"COUNT_ARREST ~ .\").setFeaturesCol(\"features\").setLabelCol(\"COUNT_ARREST\").setHandleInvalid(\"skip\")"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val lr \u003d new LinearRegression().setLabelCol(\"COUNT_ARREST\").setFeaturesCol(\"features\")\nval pipeline \u003d new Pipeline().setStages(Array(rFormula,stringIndexer, oheEncoder, vecAssembler, lr))\nval pipelineModel \u003d pipeline.fit(trainDF)\nval predDF \u003d pipelineModel.transform(testDF)\nz.show(predDF.select(\"features\", \"COUNT_ARREST\", \"prediction\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.regression.LinearRegression"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val lr \u003d new LinearRegression().setLabelCol(\"COUNT_ARREST\").setFeaturesCol(\"features\")\nval pipeline \u003d new Pipeline().setStages(Array(stringIndexer, oheEncoder, vecAssembler, lr))\nval pipelineModel \u003d pipeline.fit(trainDF)\nval predDF \u003d pipelineModel.transform(testDF)\nz.show(predDF.select(\"features\", \"COUNT_ARREST\", \"prediction\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val regressionEvaluator \u003d new RegressionEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"COUNT_ARREST\").setMetricName(\"rmse\")\nval rmse \u003d regressionEvaluator.evaluate(predDF)\nprintln(f\"RMSE is $rmse%1.2f\")\nval r2 \u003d regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\nprintln(f\"R2 is $r2%1.2f\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val rf \u003d new RandomForestRegressor().setLabelCol(\"COUNT_ARREST\").setFeaturesCol(\"features\")"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val pipeline \u003d new Pipeline().setStages(Array(stringIndexer, oheEncoder, vecAssembler, rf))\nval pipelineModel \u003d pipeline.fit(trainDF)\nval predDF \u003d pipelineModel.transform(testDF)\nz.show(predDF.select(\"features\", \"COUNT_ARREST\", \"prediction\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val regressionEvaluator \u003d new RegressionEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"COUNT_ARREST\").setMetricName(\"rmse\")\nval rmse \u003d regressionEvaluator.evaluate(predDF)\nprintln(f\"RMSE is $rmse%1.2f\")\nval r2 \u003d regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\nprintln(f\"R2 is $r2%1.2f\")"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val gbt \u003d new GBTRegressor().setLabelCol(\"COUNT_ARREST\").setFeaturesCol(\"features\").setMaxIter(10)\nval pipeline \u003d new Pipeline().setStages(Array(stringIndexer, oheEncoder, vecAssembler, gbt))\nval pipelineModel \u003d pipeline.fit(trainDF)\nval predDF \u003d pipelineModel.transform(testDF)\nz.show(predDF.select(\"features\", \"COUNT_ARREST\", \"prediction\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val regressionEvaluator \u003d new RegressionEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"COUNT_ARREST\").setMetricName(\"rmse\")\nval rmse \u003d regressionEvaluator.evaluate(predDF)\nprintln(f\"RMSE is $rmse%1.2f\")\nval r2 \u003d regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\nprintln(f\"R2 is $r2%1.2f\")"
    }
  ]
}